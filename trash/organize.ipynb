{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PET = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/petprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "PAPER = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/paperprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "OTHER = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/otherprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "IRON = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/ironprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "HDPE = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/hdpeprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "GLASS = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/glassprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "FILM = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/filmprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "CARD = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/cardboardprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "ALM = \"../4P/input/core_data_files/projected_by_linear_model_to_2050/aluminumprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "ALL_WASTE_FILES = [PET, PAPER, OTHER, IRON, HDPE, GLASS, FILM, CARD, ALM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      County           State              State_County  latitude (deg)  \\\n",
      "0  Abbeville  South Carolina  South Carolina_Abbeville       34.213809   \n",
      "1     Acadia       Louisiana          Louisiana_Acadia       30.291497   \n",
      "2   Accomack        Virginia         Virginia_Accomack       37.765944   \n",
      "3        Ada           Idaho                 Idaho_Ada       43.447861   \n",
      "4      Adair            Iowa                Iowa_Adair       41.328528   \n",
      "\n",
      "   longitude (deg)  Year                               uri          PET  \\\n",
      "0       -82.460460  2031  https://www.geonames.org/4568959   220.193758   \n",
      "1       -92.411037  2031  https://www.geonames.org/4314344   518.708380   \n",
      "2       -75.757807  2031  https://www.geonames.org/4743865   362.414202   \n",
      "3      -116.244456  2031  https://www.geonames.org/5583739  3679.000026   \n",
      "4       -94.478164  2031  https://www.geonames.org/4846344    58.721882   \n",
      "\n",
      "         Paper       Other        Iron         HDPE        Glass        Film  \\\n",
      "0   127.173331    8.007046   14.012330   120.105686    60.052843    2.429587   \n",
      "1   721.519789   28.817132   11.526853   345.805586    57.634264   15.857730   \n",
      "2   319.896936    9.884024   65.893491   263.573965   362.414202    4.339098   \n",
      "3  8275.894728  122.633334  327.022225  2861.444465  2452.666684  281.182022   \n",
      "4   431.013817    2.135341    4.804518    37.368471   112.105412    5.081118   \n",
      "\n",
      "           Card         ALM  \n",
      "0   1201.056863   20.017614  \n",
      "1   2651.176162   28.817132  \n",
      "2   1581.443789   32.946746  \n",
      "3  22891.555717  408.777781  \n",
      "4    459.098353   26.691765  \n",
      "\n",
      "Counties with missing URIs:\n",
      "Empty DataFrame\n",
      "Columns: [County, State, Year]\n",
      "Index: []\n",
      "\n",
      "Number of counties with missing URIs by state:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# we have to normalize the county names to match the URI data because \n",
    "# Miguel's scripts sucks\n",
    "def normalize_county_name(name):\n",
    "    # convert to lowercase\n",
    "    name = name.lower()\n",
    "    # replace \"saint\" with \"st\"\n",
    "    name = re.sub(r'\\bsaint\\b', 'st', name)\n",
    "    # remove periods after \"st\"\n",
    "    name = re.sub(r'\\bst\\.', 'st', name)\n",
    "    # remove other punctuation and extra spaces\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # replace Sainte with Ste\n",
    "    name = re.sub(r'\\bsainte\\b', 'ste', name)\n",
    "    # replace De Soto with Desoto\n",
    "    name = re.sub(r'\\bde soto\\b', 'desoto', name)\n",
    "    return name\n",
    "\n",
    "def combine_waste_data(file_paths, uri_file_path):\n",
    "    uri_df = pd.read_csv(uri_file_path)\n",
    "    uri_df['NormalizedCounty'] = uri_df['toponymName'].apply(normalize_county_name)\n",
    "    \n",
    "    all_data = []\n",
    "    waste_types = ['PET', 'Paper', 'Other', 'Iron', 'HDPE', 'Glass', 'Film', 'Card', 'ALM']\n",
    "    \n",
    "    for file_path, waste_type in zip(file_paths, waste_types):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Remove the 'Unnamed: 0' column if it exists\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        year_columns = df.columns[df.columns.str.isnumeric()]\n",
    "        \n",
    "        # Melt the dataframe to convert years to a single column\n",
    "        df_melted = df.melt(id_vars=['name', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)'],\n",
    "                            value_vars=year_columns,\n",
    "                            var_name='Year', value_name=waste_type)\n",
    "        \n",
    "        # Convert Year to integer\n",
    "        df_melted['Year'] = df_melted['Year'].astype(int)\n",
    "        \n",
    "        all_data.append(df_melted)\n",
    "    \n",
    "    # merge all dataframes\n",
    "    combined_df = all_data[0]\n",
    "    for df in all_data[1:]:\n",
    "        combined_df = pd.merge(combined_df, df, on=['name', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)', 'Year'])\n",
    "    \n",
    "    # self explanatory\n",
    "    combined_df = combined_df.rename(columns={'name': 'County'})\n",
    "    combined_df['NormalizedCounty'] = combined_df['County'].apply(normalize_county_name)\n",
    "    \n",
    "    # fun to find matching URI\n",
    "    def find_uri(row, uri_df):\n",
    "        state_matches = uri_df[uri_df['adminName1'] == row['State']]\n",
    "        if not state_matches.empty:\n",
    "            county_match = state_matches[state_matches['NormalizedCounty'].str.contains(row['NormalizedCounty'], case=False, na=False)]\n",
    "            if not county_match.empty:\n",
    "                return county_match.iloc[0]['uri']\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the function to each row\n",
    "    combined_df['uri'] = combined_df.apply(lambda row: find_uri(row, uri_df), axis=1)\n",
    "    \n",
    "    # Remove the temporary NormalizedCounty column\n",
    "    combined_df = combined_df.drop('NormalizedCounty', axis=1)\n",
    "    \n",
    "    # Reorder columns\n",
    "    columns_order = ['County', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)', 'Year', 'uri'] + waste_types\n",
    "    combined_df = combined_df[columns_order]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "ALL_WASTE_FILES = [PET, PAPER, OTHER, IRON, HDPE, GLASS, FILM, CARD, ALM]\n",
    "URI_FILE = 'counties_uris.csv'\n",
    "combined_waste_df = combine_waste_data(ALL_WASTE_FILES, URI_FILE)\n",
    "\n",
    "print(combined_waste_df.head())\n",
    "\n",
    "missing_uris = combined_waste_df[combined_waste_df['uri'].isna()][['County', 'State', 'Year']].drop_duplicates()\n",
    "print(\"\\nCounties with missing URIs:\")\n",
    "print(missing_uris)\n",
    "\n",
    "missing_by_state = missing_uris.groupby('State').size().sort_values(ascending=False)\n",
    "print(\"\\nNumber of counties with missing URIs by state:\")\n",
    "print(missing_by_state)\n",
    "\n",
    "combined_waste_df.to_csv('combined_waste_data_with_uri.csv', index=False)\n",
    "\n",
    "missing_uris.to_csv('missing_uris.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
