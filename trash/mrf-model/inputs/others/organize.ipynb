{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = \"../projected_by_linear_model_to_2050\"\n",
    "PET = path_folder+\"/petprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "PAPER = path_folder+\"/paperprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "OTHER = path_folder+\"/otherprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "IRON = path_folder+\"/ironprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "HDPE = path_folder+\"/hdpeprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "GLASS = path_folder+\"/glassprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "FILM = path_folder+\"/filmprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "CARD = path_folder+\"/cardboardprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "ALM = path_folder+\"/aluminumprojected_amounts_to_relog_grouped_2050.csv\"\n",
    "ALL_WASTE_FILES = [PET, PAPER, OTHER, IRON, HDPE, GLASS, FILM, CARD, ALM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      County           State              State_County  latitude (deg)  \\\n",
      "0  Abbeville  South Carolina  South Carolina_Abbeville       34.213809   \n",
      "1     Acadia       Louisiana          Louisiana_Acadia       30.291497   \n",
      "2   Accomack        Virginia         Virginia_Accomack       37.765944   \n",
      "3        Ada           Idaho                 Idaho_Ada       43.447861   \n",
      "4      Adair            Iowa                Iowa_Adair       41.328528   \n",
      "\n",
      "   longitude (deg)  Year                               uri          PET  \\\n",
      "0       -82.460460  2004  https://www.geonames.org/4568959   177.339712   \n",
      "1       -92.411037  2004  https://www.geonames.org/4314344   485.163465   \n",
      "2       -75.757807  2004  https://www.geonames.org/4743865   272.786855   \n",
      "3      -116.244456  2004  https://www.geonames.org/5583739  2564.278612   \n",
      "4       -94.478164  2004  https://www.geonames.org/4846344    58.150904   \n",
      "\n",
      "         Paper      Other        Iron         HDPE        Glass        Film  \\\n",
      "0   102.422894   6.448717   11.285254    96.730752    48.365376    1.956741   \n",
      "1   674.859044  26.953526   10.781410   323.442310    53.907052   14.832209   \n",
      "2   240.784381   7.439642   49.597610   198.390440   272.786855    3.266011   \n",
      "3  5768.333703  85.475954  227.935877  1994.438920  1709.519075  195.985061   \n",
      "4   426.822884   2.114578    4.757801    37.005121   111.015363    5.031712   \n",
      "\n",
      "           Card         ALM  \n",
      "0    967.307521   16.121792  \n",
      "1   2479.724377   26.953526  \n",
      "2   1190.342640   24.798805  \n",
      "3  15955.511362  284.919846  \n",
      "4    454.634342   26.432229  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# we have to normalize the county names to match the URI data because \n",
    "# Miguel's scripts sucks\n",
    "def normalize_county_name(name):\n",
    "    # convert to lowercase\n",
    "    name = name.lower()\n",
    "    # replace \"saint\" with \"st\"\n",
    "    name = re.sub(r'\\bsaint\\b', 'st', name)\n",
    "    # remove periods after \"st\"\n",
    "    name = re.sub(r'\\bst\\.', 'st', name)\n",
    "    # remove other punctuation and extra spaces\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # replace Sainte with Ste\n",
    "    name = re.sub(r'\\bsainte\\b', 'ste', name)\n",
    "    # replace De Soto with Desoto\n",
    "    name = re.sub(r'\\bde soto\\b', 'desoto', name)\n",
    "    return name\n",
    "\n",
    "def combine_waste_data(file_paths, uri_file_path):\n",
    "    assert Path(uri_file_path).exists(),uri_file_path.resolve()\n",
    "    uri_df = pd.read_csv(uri_file_path)\n",
    "    uri_df['NormalizedCounty'] = uri_df['toponymName'].apply(normalize_county_name)\n",
    "    \n",
    "    all_data = []\n",
    "    waste_types = ['PET', 'Paper', 'Other', 'Iron', 'HDPE', 'Glass', 'Film', 'Card', 'ALM']\n",
    "    \n",
    "    for file_path, waste_type in zip(file_paths, waste_types):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Remove the 'Unnamed: 0' and 'index' columns if these exist\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop('Unnamed: 0', axis=\"columns\")\n",
    "        if 'index' in df.columns:\n",
    "            df = df.drop('index', axis=\"columns\")\n",
    "        \n",
    "        df.columns = df.columns.str.replace('.0', '')\n",
    "        year_columns = df.columns[df.columns.str.isnumeric()]\n",
    "        \n",
    "        # Melt the dataframe to convert years to a single column\n",
    "        df_melted = df.melt(id_vars=['name', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)'],\n",
    "                            value_vars=year_columns,\n",
    "                            var_name='Year', value_name=waste_type)\n",
    "        \n",
    "        # Convert Year to integer\n",
    "        df_melted['Year'] = df_melted['Year'].astype(int)\n",
    "        \n",
    "        all_data.append(df_melted)\n",
    "    \n",
    "    # merge all dataframes\n",
    "    combined_df = all_data[0]\n",
    "    for df in all_data[1:]:\n",
    "        combined_df = pd.merge(combined_df, df, on=['name', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)', 'Year'])\n",
    "    \n",
    "    # self explanatory\n",
    "    combined_df = combined_df.rename(columns={'name': 'County'})\n",
    "    combined_df['NormalizedCounty'] = combined_df['County'].apply(normalize_county_name)\n",
    "    \n",
    "    # fun to find matching URI\n",
    "    def find_uri(row, uri_df):\n",
    "        state_matches = uri_df[uri_df['adminName1'] == row['State']]\n",
    "        if not state_matches.empty:\n",
    "            county_match = state_matches[state_matches['NormalizedCounty'].str.contains(row['NormalizedCounty'], case=False, na=False)]\n",
    "            if not county_match.empty:\n",
    "                return county_match.iloc[0]['uri']\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the function to each row\n",
    "    combined_df['uri'] = combined_df.apply(lambda row: find_uri(row, uri_df), axis=1)\n",
    "    \n",
    "    # Remove the temporary NormalizedCounty column\n",
    "    combined_df = combined_df.drop('NormalizedCounty', axis=1)\n",
    "    \n",
    "    # Reorder columns\n",
    "    columns_order = ['County', 'State', 'State_County', 'latitude (deg)', 'longitude (deg)', 'Year', 'uri'] + waste_types\n",
    "    combined_df = combined_df[columns_order]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "ALL_WASTE_FILES = [PET, PAPER, OTHER, IRON, HDPE, GLASS, FILM, CARD, ALM]\n",
    "URI_FILE = 'counties_uris.csv'\n",
    "combined_waste_df = combine_waste_data(ALL_WASTE_FILES, URI_FILE)\n",
    "\n",
    "print(combined_waste_df.head())\n",
    "\n",
    "# missing_uris = combined_waste_df[combined_waste_df['uri'].isna()][['County', 'State', 'Year']].drop_duplicates()\n",
    "# print(\"\\nCounties with missing URIs:\")\n",
    "# print(missing_uris)\n",
    "\n",
    "# missing_by_state = missing_uris.groupby('State').size().sort_values(ascending=False)\n",
    "# print(\"\\nNumber of counties with missing URIs by state:\")\n",
    "# print(missing_by_state)\n",
    "\n",
    "combined_waste_df.to_csv('combined_waste_data_with_uri.csv', index=False)\n",
    "\n",
    "# missing_uris.to_csv('missing_uris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
